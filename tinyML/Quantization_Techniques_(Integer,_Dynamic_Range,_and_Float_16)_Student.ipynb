{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LRManamperi/Machine-Learning/blob/main/tinyML/Quantization_Techniques_(Integer%2C_Dynamic_Range%2C_and_Float_16)_Student.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installing Required Packages\n",
        "\n",
        "### Note: Resolving `keras.src` Namespace Issue\n",
        "When using TensorFlow and TensorFlow Model Optimization in Colab, you may encounter a `keras.src` namespace issue, causing incompatibility with `tensorflow_model_optimization.quantization.keras`. To resolve this:\n",
        "\n",
        "1. Set the `KERAS_BACKEND` environment variable to `tensorflow` before importing TensorFlow.\n",
        "2. Ensure you are using compatible versions of TensorFlow (`>=2.12`) and TensorFlow Model Optimization.\n",
        "3. Clone the model using `tensorflow.keras.models.clone_model()` to ensure it aligns with the `tensorflow.keras` namespace.\n",
        "4. Always restart the runtime and reinstall TensorFlow-related packages to avoid lingering conflicts.\n",
        "\n",
        "This ensures that all operations use the correct `tensorflow.keras` implementation, avoiding compatibility issues.\n"
      ],
      "metadata": {
        "id": "U3RedTg8873J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y keras tensorflow tensorflow-model-optimization\n",
        "!pip install tensorflow==2.12 tensorflow-model-optimization"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ynbwI56t9ceO",
        "outputId": "d462f2ce-c9ce-476d-bc34-1b3d66c211d2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: keras 3.10.0\n",
            "Uninstalling keras-3.10.0:\n",
            "  Successfully uninstalled keras-3.10.0\n",
            "Found existing installation: tensorflow 2.19.0\n",
            "Uninstalling tensorflow-2.19.0:\n",
            "  Successfully uninstalled tensorflow-2.19.0\n",
            "\u001b[33mWARNING: Skipping tensorflow-model-optimization as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting tensorflow==2.12\n",
            "  Downloading tensorflow-2.12.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Collecting tensorflow-model-optimization\n",
            "  Downloading tensorflow_model_optimization-0.8.0-py2.py3-none-any.whl.metadata (904 bytes)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (25.2.10)\n",
            "Collecting gast<=0.4.0,>=0.2.1 (from tensorflow==2.12)\n",
            "  Downloading gast-0.4.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (1.74.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (3.14.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (0.5.3)\n",
            "Collecting keras<2.13,>=2.12.0 (from tensorflow==2.12)\n",
            "  Downloading keras-2.12.0-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (18.1.1)\n",
            "Collecting numpy<1.24,>=1.22 (from tensorflow==2.12)\n",
            "  Downloading numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (25.0)\n",
            "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow==2.12)\n",
            "  Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (1.17.0)\n",
            "Collecting tensorboard<2.13,>=2.12 (from tensorflow==2.12)\n",
            "  Downloading tensorboard-2.12.3-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting tensorflow-estimator<2.13,>=2.12.0 (from tensorflow==2.12)\n",
            "  Downloading tensorflow_estimator-2.12.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (4.14.1)\n",
            "Collecting wrapt<1.15,>=1.11.0 (from tensorflow==2.12)\n",
            "  Downloading wrapt-1.14.2-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (0.37.1)\n",
            "Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow-model-optimization) (0.1.9)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow==2.12) (0.45.1)\n",
            "Requirement already satisfied: attrs>=18.2.0 in /usr/local/lib/python3.11/dist-packages (from dm-tree~=0.1.1->tensorflow-model-optimization) (25.3.0)\n",
            "Requirement already satisfied: jaxlib<=0.5.3,>=0.5.3 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow==2.12) (0.5.3)\n",
            "Requirement already satisfied: ml_dtypes>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow==2.12) (0.5.3)\n",
            "INFO: pip is looking at multiple versions of jax to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12)\n",
            "  Downloading jax-0.7.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting jaxlib<=0.7.0,>=0.7.0 (from jax>=0.3.15->tensorflow==2.12)\n",
            "  Downloading jaxlib-0.7.0-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12)\n",
            "  Downloading jax-0.6.2-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting jaxlib<=0.6.2,>=0.6.2 (from jax>=0.3.15->tensorflow==2.12)\n",
            "  Downloading jaxlib-0.6.2-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12)\n",
            "  Downloading jax-0.6.1-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting jaxlib<=0.6.1,>=0.6.1 (from jax>=0.3.15->tensorflow==2.12)\n",
            "  Downloading jaxlib-0.6.1-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12)\n",
            "  Downloading jax-0.6.0-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.6.0,>=0.6.0 (from jax>=0.3.15->tensorflow==2.12)\n",
            "  Downloading jaxlib-0.6.0-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12)\n",
            "  Downloading jax-0.5.2-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.5.2,>=0.5.1 (from jax>=0.3.15->tensorflow==2.12)\n",
            "  Downloading jaxlib-0.5.1-cp311-cp311-manylinux2014_x86_64.whl.metadata (978 bytes)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12)\n",
            "  Downloading jax-0.5.1-py3-none-any.whl.metadata (22 kB)\n",
            "  Downloading jax-0.5.0-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.5.0,>=0.5.0 (from jax>=0.3.15->tensorflow==2.12)\n",
            "  Downloading jaxlib-0.5.0-cp311-cp311-manylinux2014_x86_64.whl.metadata (978 bytes)\n",
            "INFO: pip is still looking at multiple versions of jax to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12)\n",
            "  Downloading jax-0.4.38-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.38,>=0.4.38 (from jax>=0.3.15->tensorflow==2.12)\n",
            "  Downloading jaxlib-0.4.38-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12)\n",
            "  Downloading jax-0.4.37-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.37,>=0.4.36 (from jax>=0.3.15->tensorflow==2.12)\n",
            "  Downloading jaxlib-0.4.36-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12)\n",
            "  Downloading jax-0.4.36-py3-none-any.whl.metadata (22 kB)\n",
            "  Downloading jax-0.4.35-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.35,>=0.4.34 (from jax>=0.3.15->tensorflow==2.12)\n",
            "  Downloading jaxlib-0.4.35-cp311-cp311-manylinux2014_x86_64.whl.metadata (983 bytes)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12)\n",
            "  Downloading jax-0.4.34-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.34,>=0.4.34 (from jax>=0.3.15->tensorflow==2.12)\n",
            "  Downloading jaxlib-0.4.34-cp311-cp311-manylinux2014_x86_64.whl.metadata (983 bytes)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12)\n",
            "  Downloading jax-0.4.33-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.33,>=0.4.33 (from jax>=0.3.15->tensorflow==2.12)\n",
            "  Downloading jaxlib-0.4.33-cp311-cp311-manylinux2014_x86_64.whl.metadata (983 bytes)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12)\n",
            "  Downloading jax-0.4.31-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.31,>=0.4.30 (from jax>=0.3.15->tensorflow==2.12)\n",
            "  Downloading jaxlib-0.4.31-cp311-cp311-manylinux2014_x86_64.whl.metadata (983 bytes)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12)\n",
            "  Downloading jax-0.4.30-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.30,>=0.4.27 (from jax>=0.3.15->tensorflow==2.12)\n",
            "  Downloading jaxlib-0.4.30-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow==2.12) (1.16.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12) (2.38.0)\n",
            "Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard<2.13,>=2.12->tensorflow==2.12)\n",
            "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12) (3.8.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12) (3.1.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12) (4.9.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12) (2.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12) (2025.8.3)\n",
            "INFO: pip is looking at multiple versions of scipy to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting scipy>=1.9 (from jax>=0.3.15->tensorflow==2.12)\n",
            "  Downloading scipy-1.16.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.9/61.9 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading scipy-1.15.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow==2.12) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12) (3.3.1)\n",
            "Downloading tensorflow-2.12.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (586.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m586.0/586.0 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_model_optimization-0.8.0-py2.py3-none-any.whl (242 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.5/242.5 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "Downloading jax-0.4.30-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading keras-2.12.0-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m78.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m103.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.12.3-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m118.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_estimator-2.12.0-py2.py3-none-any.whl (440 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.7/440.7 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wrapt-1.14.2-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.4/77.4 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
            "Downloading jaxlib-0.4.30-cp311-cp311-manylinux2014_x86_64.whl (79.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.6/79.6 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.15.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.7/37.7 MB\u001b[0m \u001b[31m52.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: wrapt, tensorflow-estimator, protobuf, numpy, keras, gast, scipy, tensorflow-model-optimization, jaxlib, google-auth-oauthlib, tensorboard, jax, tensorflow\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.17.2\n",
            "    Uninstalling wrapt-1.17.2:\n",
            "      Successfully uninstalled wrapt-1.17.2\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.29.5\n",
            "    Uninstalling protobuf-5.29.5:\n",
            "      Successfully uninstalled protobuf-5.29.5\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.6.0\n",
            "    Uninstalling gast-0.6.0:\n",
            "      Successfully uninstalled gast-0.6.0\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.16.1\n",
            "    Uninstalling scipy-1.16.1:\n",
            "      Successfully uninstalled scipy-1.16.1\n",
            "  Attempting uninstall: jaxlib\n",
            "    Found existing installation: jaxlib 0.5.3\n",
            "    Uninstalling jaxlib-0.5.3:\n",
            "      Successfully uninstalled jaxlib-0.5.3\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 1.2.2\n",
            "    Uninstalling google-auth-oauthlib-1.2.2:\n",
            "      Successfully uninstalled google-auth-oauthlib-1.2.2\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.19.0\n",
            "    Uninstalling tensorboard-2.19.0:\n",
            "      Successfully uninstalled tensorboard-2.19.0\n",
            "  Attempting uninstall: jax\n",
            "    Found existing installation: jax 0.5.3\n",
            "    Uninstalling jax-0.5.3:\n",
            "      Successfully uninstalled jax-0.5.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tf-keras 2.19.0 requires tensorflow<2.20,>=2.19, but you have tensorflow 2.12.0 which is incompatible.\n",
            "albumentations 2.0.8 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "bigframes 2.14.0 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\n",
            "albucore 0.0.24 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "ydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 4.25.8 which is incompatible.\n",
            "pywavelets 1.9.0 requires numpy<3,>=1.25, but you have numpy 1.23.5 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.23.5 which is incompatible.\n",
            "chex 0.1.90 requires numpy>=1.24.1, but you have numpy 1.23.5 which is incompatible.\n",
            "geopandas 1.1.1 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "imbalanced-learn 0.13.0 requires numpy<3,>=1.24.3, but you have numpy 1.23.5 which is incompatible.\n",
            "contourpy 1.3.3 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\n",
            "keras-hub 0.21.1 requires keras>=3.5, but you have keras 2.12.0 which is incompatible.\n",
            "xarray-einstats 0.9.1 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\n",
            "xarray 2025.7.1 requires numpy>=1.26, but you have numpy 1.23.5 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.23.5 which is incompatible.\n",
            "tensorflow-text 2.19.0 requires tensorflow<2.20,>=2.19.0, but you have tensorflow 2.12.0 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.23.5 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.23.5 which is incompatible.\n",
            "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.8 which is incompatible.\n",
            "db-dtypes 1.4.3 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\n",
            "arviz 0.22.0 requires numpy>=1.26.0, but you have numpy 1.23.5 which is incompatible.\n",
            "blosc2 3.6.1 requires numpy>=1.26, but you have numpy 1.23.5 which is incompatible.\n",
            "scikit-image 0.25.2 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "orbax-checkpoint 0.11.21 requires jax>=0.5.0, but you have jax 0.4.30 which is incompatible.\n",
            "treescope 0.1.10 requires numpy>=1.25.2, but you have numpy 1.23.5 which is incompatible.\n",
            "tensorflow-decision-forests 1.12.0 requires tensorflow==2.19.0, but you have tensorflow 2.12.0 which is incompatible.\n",
            "flax 0.10.6 requires jax>=0.5.1, but you have jax 0.4.30 which is incompatible.\n",
            "pymc 5.25.1 requires numpy>=1.25.0, but you have numpy 1.23.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gast-0.4.0 google-auth-oauthlib-1.0.0 jax-0.4.30 jaxlib-0.4.30 keras-2.12.0 numpy-1.23.5 protobuf-4.25.8 scipy-1.15.3 tensorboard-2.12.3 tensorflow-2.12.0 tensorflow-estimator-2.12.0 tensorflow-model-optimization-0.8.0 wrapt-1.14.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google",
                  "numpy"
                ]
              },
              "id": "f91ad840bf834370a6fc45b289106962"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "\n",
        "import os\n",
        "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model, Input\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense"
      ],
      "metadata": {
        "id": "CFEj5v-J9pPz"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part I:  Post Training Quantization (PTQ) - Integer, Dynamic Range, and Float 16\n",
        "First part of this notebook demonstrates three types of post-training quantization for a CNN model trained on the MNIST dataset. Quantization is a model compression technique that reduces model size and computational requirements, enabling efficient deployment on resource-constrained devices.\n",
        "\n",
        "## Goals:\n",
        "1. Train a CNN model on the MNIST dataset.\n",
        "2. Apply three quantization techniques:\n",
        "   - Integer Quantization\n",
        "   - Dynamic Range Quantization\n",
        "   - Float 16 Quantization\n",
        "3. Compare the quantized models in terms of size and accuracy.\n"
      ],
      "metadata": {
        "id": "omSnTckQoK9e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Preparation\n",
        "We use the MNIST dataset, which contains grayscale images of handwritten digits (0-9).\n",
        "1. Normalize the pixel values to the range [0, 1].\n",
        "2. Reshape the data for input into the CNN model.\n",
        "3. One-hot encode the labels for classification.\n"
      ],
      "metadata": {
        "id": "oSVayjgGoaUu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mF3L0T0CoJco",
        "outputId": "358cbe73-b028-40cb-da21-39fc035121e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 2s 0us/step\n",
            "Training data shape: (60000, 28, 28, 1), Labels shape: (60000, 10)\n",
            "Test data shape: (10000, 28, 28, 1), Labels shape: (10000, 10)\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load the MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Normalize the data\n",
        "x_train = x_train.astype(\"float32\") / 255.0\n",
        "x_test = x_test.astype(\"float32\") / 255.0\n",
        "\n",
        "# Reshape the data for CNN input\n",
        "x_train = x_train.reshape(-1, 28, 28, 1)\n",
        "x_test = x_test.reshape(-1, 28, 28, 1)\n",
        "\n",
        "# One-hot encode the labels\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "print(f\"Training data shape: {x_train.shape}, Labels shape: {y_train.shape}\")\n",
        "print(f\"Test data shape: {x_test.shape}, Labels shape: {y_test.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training a Simple CNN\n",
        "We build a Convolutional Neural Network (CNN) with the following layers:\n",
        "1. **Convolutional Layer**: Extracts features from the input images.\n",
        "2. **MaxPooling Layer**: Reduces spatial dimensions, lowering computational requirements.\n",
        "3. **Flatten Layer**: Converts the 2D feature maps into a 1D vector.\n",
        "4. **Dense Layers**: Fully connected layers for classification.\n",
        "\n",
        "The model is compiled using the Adam optimizer and trained for 2 epochs.\n"
      ],
      "metadata": {
        "id": "oF0-Yeerolqs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a simple CNN model\n",
        "def create_cnn_model():\n",
        "    inputs = Input(shape=(28, 28, 1))\n",
        "    x = Conv2D(32, (3, 3), activation='relu')(inputs)\n",
        "    x = MaxPooling2D((2, 2))(x)\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(128, activation='relu')(x)\n",
        "    outputs = Dense(10, activation='softmax')(x)\n",
        "    return Model(inputs, outputs)\n",
        "\n",
        "# Compile and train the model\n",
        "model = create_cnn_model()\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "history = model.fit(x_train, y_train, epochs=2, batch_size=32, validation_data=(x_test, y_test))\n",
        "\n",
        "# Evaluate the trained model\n",
        "original_accuracy = model.evaluate(x_test, y_test, verbose=0)[1]\n",
        "print(f\"Original Model Accuracy: {original_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ccszLiRovPM",
        "outputId": "64550fde-f657-4679-c326-425517f9aea2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "1875/1875 [==============================] - 37s 19ms/step - loss: 0.1457 - accuracy: 0.9566 - val_loss: 0.0622 - val_accuracy: 0.9794\n",
            "Epoch 2/2\n",
            "1875/1875 [==============================] - 36s 19ms/step - loss: 0.0484 - accuracy: 0.9851 - val_loss: 0.0422 - val_accuracy: 0.9859\n",
            "Original Model Accuracy: 0.9859\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Applying Quantization Techniques\n",
        "We apply three types of post-training quantization to the trained CNN model:\n",
        "1. **Integer Quantization**: Converts both weights and activations to 8-bit integers.\n",
        "2. **Dynamic Range Quantization**: Reduces the precision of weights while keeping activations in floating-point format.\n",
        "3. **Float 16 Quantization**: Converts both weights and activations to 16-bit floating-point values.\n"
      ],
      "metadata": {
        "id": "b81MxPHmpnCV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------\n",
        "# BASELINE: No quantization\n",
        "# ------------------------------\n",
        "\n",
        "# Convert the original Keras model to TFLite format without any quantization\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save the baseline model to a .tflite file\n",
        "with open(\"model_baseline.tflite\", \"wb\") as f:\n",
        "    f.write(tflite_model)\n",
        "\n",
        "# ------------------------------\n",
        "# FULL INTEGER QUANTIZATION\n",
        "# ------------------------------\n",
        "\n",
        "# Define a representative dataset generator required for calibrating quantization\n",
        "# Note: input data must match model input shape and dtype (float32)\n",
        "def representative_data_gen():\n",
        "    for input_value in x_test[:100]:\n",
        "        yield [input_value.reshape(1, 28, 28, 1).astype(\"float32\")]\n",
        "\n",
        "# Create the converter from the Keras model again\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "\n",
        "# Enable default optimizations (includes quantization)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "\n",
        "# Provide representative dataset for calibration\n",
        "converter.representative_dataset = representative_data_gen\n",
        "\n",
        "# Specify that only int8 operations are to be used in the model\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "\n",
        "# Set input and output tensor types to int8\n",
        "converter.inference_input_type = tf.int8\n",
        "converter.inference_output_type = tf.int8\n",
        "\n",
        "# Convert and save the INT8 quantized model\n",
        "tflite_model_int8 = converter.convert()\n",
        "with open(\"model_integer_quant.tflite\", \"wb\") as f:\n",
        "    f.write(tflite_model_int8)\n",
        "\n",
        "# ------------------------------\n",
        "# DYNAMIC RANGE QUANTIZATION\n",
        "# ------------------------------\n",
        "\n",
        "# This quantization method does not require a representative dataset\n",
        "# Only weights are quantized; activations stay in float\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "\n",
        "# Convert and save the dynamic range quantized model\n",
        "tflite_model_dynamic = converter.convert()\n",
        "with open(\"model_dynamic_quant.tflite\", \"wb\") as f:\n",
        "    f.write(tflite_model_dynamic)\n",
        "\n",
        "# ------------------------------\n",
        "# FLOAT16 QUANTIZATION\n",
        "# ------------------------------\n",
        "\n",
        "# Convert the model to use float16 weights (useful for GPUs or TPUs that support float16)\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "\n",
        "# Set the supported types to float16\n",
        "converter.target_spec.supported_types = [tf.float16]\n",
        "\n",
        "# Convert and save the float16 quantized model\n",
        "tflite_model_float16 = converter.convert()\n",
        "with open(\"model_float16_quant.tflite\", \"wb\") as f:\n",
        "    f.write(tflite_model_float16)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GbW9QzgOpwb5",
        "outputId": "c154561a-8285-4db5-dfc8-17dd0c8f671c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n",
            "/usr/local/lib/python3.11/dist-packages/tensorflow/lite/python/convert.py:789: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
            "  warnings.warn(\"Statistics for quantized inputs were expected, but not \"\n",
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comparison of Model Sizes and Accuracy\n",
        "We compare the quantized models in terms of:\n",
        "1. **Model Size**: Smaller models are better suited for resource-constrained devices.\n",
        "2. **Accuracy**: Quantization should preserve the original model’s accuracy as much as possible.\n"
      ],
      "metadata": {
        "id": "Bgr-nMzxsktu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf  # Ensure TensorFlow is imported\n",
        "\n",
        "# Compare model sizes\n",
        "model_files = [\n",
        "    \"model_baseline.tflite\",\n",
        "    \"model_integer_quant.tflite\",\n",
        "    \"model_dynamic_quant.tflite\",\n",
        "    \"model_float16_quant.tflite\"\n",
        "]\n",
        "\n",
        "print(\"Model Sizes (KB):\")\n",
        "for file in model_files:\n",
        "    print(f\"{file}: {os.path.getsize(file) / 1024:.2f} KB\")\n",
        "\n",
        "# Evaluate accuracy of a TFLite model on the test set\n",
        "def evaluate_tflite_model(tflite_model_path):\n",
        "    # Load the TFLite model and allocate tensors\n",
        "    interpreter = tf.lite.Interpreter(model_path=tflite_model_path)\n",
        "    interpreter.allocate_tensors()\n",
        "\n",
        "    # Get input and output tensor details\n",
        "    input_details = interpreter.get_input_details()\n",
        "    output_details = interpreter.get_output_details()\n",
        "\n",
        "    input_dtype = input_details[0]['dtype']\n",
        "    input_scale, input_zero_point = input_details[0]['quantization']\n",
        "\n",
        "    correct_predictions = 0\n",
        "\n",
        "    for i in range(len(x_test)):\n",
        "        # Prepare one input sample (shape: [1, 28, 28, 1])\n",
        "        input_data = x_test[i:i+1].astype(\"float32\")\n",
        "\n",
        "        # Quantize the input if model expects int8\n",
        "        if input_dtype == np.int8:\n",
        "            input_data = input_data / input_scale + input_zero_point\n",
        "            input_data = np.round(input_data).astype(np.int8)\n",
        "        elif input_dtype == np.float16:\n",
        "            input_data = input_data.astype(np.float16)\n",
        "\n",
        "        # Set the tensor and invoke the interpreter\n",
        "        interpreter.set_tensor(input_details[0]['index'], input_data)\n",
        "        interpreter.invoke()\n",
        "\n",
        "        # Get prediction and compare to true label\n",
        "        output_data = interpreter.get_tensor(output_details[0]['index'])\n",
        "        predicted_label = np.argmax(output_data, axis=1)\n",
        "        true_label = np.argmax(y_test[i:i+1], axis=1)\n",
        "\n",
        "        if predicted_label == true_label:\n",
        "            correct_predictions += 1\n",
        "\n",
        "    # Return classification accuracy\n",
        "    return correct_predictions / len(x_test)\n",
        "\n",
        "# Display accuracy of each model\n",
        "print(\"\\nModel Accuracies:\")\n",
        "print(f\"Baseline: {original_accuracy:.4f}\")\n",
        "print(f\"Integer Quantization: {evaluate_tflite_model('model_integer_quant.tflite'):.4f}\")\n",
        "print(f\"Dynamic Range Quantization: {evaluate_tflite_model('model_dynamic_quant.tflite'):.4f}\")\n",
        "print(f\"Float 16 Quantization: {evaluate_tflite_model('model_float16_quant.tflite'):.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3VzFgSfSsk7x",
        "outputId": "4cfeb08f-eb41-49e2-c81a-4bdd87938cee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Sizes (KB):\n",
            "model_baseline.tflite: 2713.34 KB\n",
            "model_integer_quant.tflite: 681.92 KB\n",
            "model_dynamic_quant.tflite: 681.70 KB\n",
            "model_float16_quant.tflite: 1358.77 KB\n",
            "\n",
            "Model Accuracies:\n",
            "Baseline: 0.9859\n",
            "Integer Quantization: 0.9857\n",
            "Dynamic Range Quantization: 0.9858\n",
            "Float 16 Quantization: 0.9859\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part I Summary\n",
        "Quantization significantly reduces model size while maintaining comparable accuracy. Key observations:\n",
        "- Integer Quantization provides the smallest model size and computation complexity but may slightly reduce accuracy.\n",
        "- Dynamic Range Quantization balances size, computation complexity, and performance.\n",
        "- Float 16 Quantization retains higher accuracy with moderate size and computation complexity reduction.\n"
      ],
      "metadata": {
        "id": "YSdtfn0tw4Wo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part II: Quantization-Aware Training (QAT) - Integer and Dynamic Range\n",
        "Quantization-Aware Training is a technique where integer quantization is simulated during the training process. This allows the model to adjust its weights and activations, minimizing the accuracy loss caused by quantization.\n",
        "\n",
        "In this step, we:\n",
        "1. Prepare a quantization-aware model using TensorFlow’s `QuantizeWrapper`.\n",
        "2. Train the model on MNIST with quantization simulation.\n",
        "3. Convert the model to TensorFlow Lite format for Integer and Dynamic Range.\n",
        "4. Compare model sizes and accuracy after QAT.\n"
      ],
      "metadata": {
        "id": "iJ53VME51G6Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify the model type\n",
        "model = create_cnn_model()\n",
        "print(f\"Model type: {type(model)}\")\n",
        "\n",
        "from tensorflow_model_optimization.quantization.keras import quantize_model\n",
        "\n",
        "# Prepare the quantization-aware model\n",
        "qat_model = quantize_model(create_cnn_model())\n",
        "\n",
        "# Compile the QAT model\n",
        "qat_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the QAT model\n",
        "print(\"Training the QAT Model...\")\n",
        "qat_history = qat_model.fit(x_train, y_train, epochs=2, batch_size=32, validation_data=(x_test, y_test))\n",
        "\n",
        "# Evaluate the QAT-trained model\n",
        "qat_accuracy = qat_model.evaluate(x_test, y_test, verbose=0)[1]\n",
        "print(f\"QAT Model Accuracy: {qat_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IV7GtpH9_RNK",
        "outputId": "513cb752-a776-4b3e-bd6b-55ea94a4a1ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model type: <class 'keras.engine.functional.Functional'>\n",
            "Training the QAT Model...\n",
            "Epoch 1/2\n",
            "1875/1875 [==============================] - 63s 33ms/step - loss: 0.1567 - accuracy: 0.9538 - val_loss: 0.0607 - val_accuracy: 0.9791\n",
            "Epoch 2/2\n",
            "1875/1875 [==============================] - 49s 26ms/step - loss: 0.0531 - accuracy: 0.9836 - val_loss: 0.0481 - val_accuracy: 0.9830\n",
            "QAT Model Accuracy: 0.9830\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Applying Quantization After QAT\n",
        "We now convert the QAT-trained model to TensorFlow Lite format and apply three quantization techniques:\n",
        "1. **Integer Quantization**\n",
        "2. **Dynamic Range Quantization**\n"
      ],
      "metadata": {
        "id": "cDLPWBrQBsZ9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### 🧪 Lab 2 Submission Reminder\n",
        "\n",
        "**Please complete the code below and take a screenshot of it as part of your Lab 2 submission.**\n",
        "\n",
        "⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "XV0GcUCe47zW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the QAT-trained Baseline model to TensorFlow Lite\n",
        "converter = #<--- Enter Your Code Here --->#\n",
        "tflite_model = #<--- Enter Your Code Here --->#\n",
        "with open(\"qat_model_baseline.tflite\", \"wb\") as f:\n",
        "    f.write(tflite_model)\n",
        "\n",
        "# Integer Quantization\n",
        "converter = #<--- Enter Your Code Here --->#\n",
        "converter.optimizations = #<--- Enter Your Code Here --->#\n",
        "converter.representative_dataset = #<--- Enter Your Code Here --->#\n",
        "converter.target_spec.supported_types = #<--- Enter Your Code Here --->#\n",
        "converter.inference_input_type = #<--- Enter Your Code Here --->#\n",
        "converter.inference_output_type = #<--- Enter Your Code Here --->#\n",
        "tflite_model_qat_int8 = #<--- Enter Your Code Here --->#\n",
        "\n",
        "# Save Integer Quantization model\n",
        "with open(\"qat_model_integer_quant.tflite\", \"wb\") as f:\n",
        "    f.write(tflite_model_qat_int8)\n",
        "\n",
        "# Dynamic Range Quantization\n",
        "converter = #<--- Enter Your Code Here --->#\n",
        "converter.optimizations = #<--- Enter Your Code Here --->#\n",
        "tflite_model_qat_dynamic = #<--- Enter Your Code Here --->#\n",
        "\n",
        "# Save Dynamic Range Quantization model\n",
        "with open(\"qat_model_dynamic_quant.tflite\", \"wb\") as f:\n",
        "    f.write(tflite_model_qat_dynamic)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "2e8kYLIiB3K6",
        "outputId": "100ea31c-d1fd-480b-821f-96b67803293a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-2162832544.py, line 2)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-2162832544.py\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    converter = #<--- Enter Your Code Here --->#\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comparison of Model Sizes and Accuracy\n",
        "We compare the model sizes and accuracy for the quantized QAT-trained models.\n"
      ],
      "metadata": {
        "id": "eVaR7rd52RG6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare model sizes\n",
        "model_files = [\n",
        "    \"qat_model_baseline.tflite\",\n",
        "    \"qat_model_integer_quant.tflite\",\n",
        "    \"qat_model_dynamic_quant.tflite\",\n",
        "]\n",
        "\n",
        "\n",
        "print(\"\\nModel Sizes After QAT (KB):\")\n",
        "for file in model_files:\n",
        "    print(f\"{file}: {os.path.getsize(file) / 1024:.2f} KB\")\n",
        "\n",
        "# Evaluate accuracy for QAT-quantized models\n",
        "print(\"\\nModel Accuracies After QAT:\")\n",
        "print(f\"QAT Baseline Model Accuracy: {qat_accuracy:.4f}\")\n",
        "print(f\"Integer Quantization: {evaluate_tflite_model('qat_model_integer_quant.tflite'):.4f}\")\n",
        "print(f\"Dynamic Range Quantization: {evaluate_tflite_model('qat_model_dynamic_quant.tflite'):.4f}\")"
      ],
      "metadata": {
        "id": "2nfH8mJLDPYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part II Summary\n",
        "Quantization-Aware Training (QAT) significantly reduces accuracy loss caused by quantization. Key observations:\n",
        "- QAT improves the accuracy of quantized models, making it suitable for resource-constrained deployments that demands the high precision/accuracy.\n",
        "- QAT is incompatible with Float16 quantization because they target fundamentally different quantization formats.\n"
      ],
      "metadata": {
        "id": "d3EG8r-e2L-1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Vu76ZeuoFaZV"
      }
    }
  ]
}