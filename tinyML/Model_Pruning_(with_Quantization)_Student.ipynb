{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LRManamperi/Machine-Learning/blob/main/tinyML/Model_Pruning_(with_Quantization)_Student.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installing Required Packages\n",
        "\n",
        "### Note: Resolving `keras.src` Namespace Issue\n",
        "When using TensorFlow and TensorFlow Model Optimization in Colab, you may encounter a `keras.src` namespace issue, causing incompatibility with `tensorflow_model_optimization.quantization.keras`. To resolve this:\n",
        "\n",
        "1. Set the `KERAS_BACKEND` environment variable to `tensorflow` before importing TensorFlow.\n",
        "2. Ensure you are using compatible versions of TensorFlow (`>=2.12`) and TensorFlow Model Optimization.\n",
        "3. Clone the model using `tensorflow.keras.models.clone_model()` to ensure it aligns with the `tensorflow.keras` namespace.\n",
        "4. Always restart the runtime and reinstall TensorFlow-related packages to avoid lingering conflicts.\n",
        "\n",
        "This ensures that all operations use the correct `tensorflow.keras` implementation, avoiding compatibility issues.\n"
      ],
      "metadata": {
        "id": "08zHoHPwV_Lw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y keras tensorflow tensorflow-model-optimization\n",
        "!pip install tensorflow==2.12 tensorflow-model-optimization"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w0aZHRPVWGF7",
        "outputId": "f73076d7-4dde-4594-b706-f74a8d8168d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: keras 2.12.0\n",
            "Uninstalling keras-2.12.0:\n",
            "  Successfully uninstalled keras-2.12.0\n",
            "Found existing installation: tensorflow 2.12.0\n",
            "Uninstalling tensorflow-2.12.0:\n",
            "  Successfully uninstalled tensorflow-2.12.0\n",
            "Found existing installation: tensorflow-model-optimization 0.8.0\n",
            "Uninstalling tensorflow-model-optimization-0.8.0:\n",
            "  Successfully uninstalled tensorflow-model-optimization-0.8.0\n",
            "Collecting tensorflow==2.12\n",
            "  Using cached tensorflow-2.12.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Collecting tensorflow-model-optimization\n",
            "  Using cached tensorflow_model_optimization-0.8.0-py2.py3-none-any.whl.metadata (904 bytes)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (25.2.10)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (1.74.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (3.14.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (0.4.30)\n",
            "Collecting keras<2.13,>=2.12.0 (from tensorflow==2.12)\n",
            "  Using cached keras-2.12.0-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (18.1.1)\n",
            "Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (4.25.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (1.17.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (2.12.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (4.14.1)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (1.14.2)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (0.37.1)\n",
            "Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow-model-optimization) (0.1.9)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow==2.12) (0.45.1)\n",
            "Requirement already satisfied: attrs>=18.2.0 in /usr/local/lib/python3.11/dist-packages (from dm-tree~=0.1.1->tensorflow-model-optimization) (25.3.0)\n",
            "Requirement already satisfied: jaxlib<=0.4.30,>=0.4.27 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow==2.12) (0.4.30)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow==2.12) (0.5.3)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow==2.12) (1.15.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12) (2.38.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12) (3.8.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12) (3.1.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12) (4.9.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12) (2.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12) (2025.8.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow==2.12) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12) (3.3.1)\n",
            "Using cached tensorflow-2.12.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (586.0 MB)\n",
            "Using cached tensorflow_model_optimization-0.8.0-py2.py3-none-any.whl (242 kB)\n",
            "Using cached keras-2.12.0-py2.py3-none-any.whl (1.7 MB)\n",
            "Installing collected packages: keras, tensorflow-model-optimization, tensorflow\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-decision-forests 1.12.0 requires tensorflow==2.19.0, but you have tensorflow 2.12.0 which is incompatible.\n",
            "tensorflow-text 2.19.0 requires tensorflow<2.20,>=2.19.0, but you have tensorflow 2.12.0 which is incompatible.\n",
            "tf-keras 2.19.0 requires tensorflow<2.20,>=2.19, but you have tensorflow 2.12.0 which is incompatible.\n",
            "keras-hub 0.21.1 requires keras>=3.5, but you have keras 2.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed keras-2.12.0 tensorflow-2.12.0 tensorflow-model-optimization-0.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "\n",
        "import os\n",
        "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model, Input\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense"
      ],
      "metadata": {
        "id": "GNfEPNO4WwV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part I: Model Pruning with Sparsity\n",
        "This part of the notebook demonstrates how pruning can be used to reduce the size of a model by removing insignificant weights. However, pruning alone does not always lead to size reductions unless additional steps are taken:\n",
        "1. Remove the pruning mask to finalize the pruned model.\n",
        "2. Use TensorFlow Lite's **experimental sparsity-aware optimization** when converting to TFLite format.\n",
        "\n",
        "## Objectives:\n",
        "1. Compare model size before and after removing the pruning mask.\n",
        "2. Show the impact of experimental sparsity optimization on reducing the TFLite model size.\n"
      ],
      "metadata": {
        "id": "urz4uV3kTCBy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Preparation\n",
        "We use the MNIST dataset, which contains grayscale images of handwritten digits (0-9).\n",
        "1. Normalize the pixel values to the range [0, 1].\n",
        "2. Reshape the data for input into the CNN model.\n",
        "3. One-hot encode the labels for classification."
      ],
      "metadata": {
        "id": "Ne2XWdVjTZLm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSDB1IRnJ5BP",
        "outputId": "d9f22221-a9f0-47c2-d1a3-8da8e13fb2c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data shape: (60000, 28, 28, 1), Labels shape: (60000, 10)\n",
            "Test data shape: (10000, 28, 28, 1), Labels shape: (10000, 10)\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load and preprocess the MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train.astype(\"float32\") / 255.0\n",
        "x_test = x_test.astype(\"float32\") / 255.0\n",
        "x_train = x_train.reshape(-1, 28, 28, 1)\n",
        "x_test = x_test.reshape(-1, 28, 28, 1)\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "print(f\"Training data shape: {x_train.shape}, Labels shape: {y_train.shape}\")\n",
        "print(f\"Test data shape: {x_test.shape}, Labels shape: {y_test.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training a Simple CNN\n",
        "We build a Convolutional Neural Network (CNN) with the following layers:\n",
        "1. **Convolutional Layer**: Extracts features from the input images.\n",
        "2. **MaxPooling Layer**: Reduces spatial dimensions, lowering computational requirements.\n",
        "3. **Flatten Layer**: Converts the 2D feature maps into a 1D vector.\n",
        "4. **Dense Layers**: Fully connected layers for classification.\n",
        "\n",
        "The model is compiled using the Adam optimizer and trained for 2 epochs.\n"
      ],
      "metadata": {
        "id": "cQVFmR1RTbvF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a simple CNN model\n",
        "def create_cnn_model():\n",
        "    inputs = Input(shape=(28, 28, 1))\n",
        "    x = Conv2D(32, (3, 3), activation='relu')(inputs)\n",
        "    x = MaxPooling2D((2, 2))(x)\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(128, activation='relu')(x)\n",
        "    outputs = Dense(10, activation='softmax')(x)\n",
        "    return Model(inputs, outputs)\n",
        "\n",
        "# Compile and train the model\n",
        "model = create_cnn_model()\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "history = model.fit(x_train, y_train, epochs=2, batch_size=32, validation_data=(x_test, y_test))\n",
        "\n",
        "# Evaluate the trained model\n",
        "baseline_accuracy = model.evaluate(x_test, y_test, verbose=0)[1]\n",
        "print(f\"Baseline Model Accuracy: {baseline_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "ZL5KvhW_TYcf",
        "outputId": "e9336217-d60a-431f-8eca-6c23c21527cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "\u001b[1m 557/1875\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m26s\u001b[0m 20ms/step - accuracy: 0.8407 - loss: 0.5367"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3664490601.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_cnn_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Evaluate the trained model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    375\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDistributedIterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             ):\n\u001b[0;32m--> 220\u001b[0;31m                 \u001b[0mopt_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulti_step_on_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopt_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    876\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m       results = tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    879\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1321\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1686\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1688\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1689\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1690\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pruning the Model\n",
        "We apply pruning using TensorFlow Model Optimization. This process sparsifies the model by removing insignificant weights while maintaining comparable accuracy.\n",
        "\n",
        "### 🔍 Explanation of `pruning_params`\n",
        "\n",
        "The `pruning_params` dictionary configures how pruning is applied using a `PolynomialDecay` schedule:\n",
        "\n",
        "- **`initial_sparsity`**:  \n",
        "  The starting proportion of zero weights when pruning begins.  \n",
        "  **Example**: `0.5` means 50% of the weights will be pruned at `begin_step`.\n",
        "\n",
        "- **`final_sparsity`**:  \n",
        "  The target proportion of zero weights to reach by `end_step`.  \n",
        "  **Example**: `0.9` means 90% of the weights will be pruned by the end of the schedule.\n",
        "\n",
        "- **`begin_step`**:  \n",
        "  The training step at which pruning begins.  \n",
        "  Typically set to `0` to start pruning from the beginning of training.\n",
        "\n",
        "- **`end_step`**:  \n",
        "  The training step at which pruning ends.  \n",
        "  After this step, the sparsity level remains fixed at `final_sparsity`.\n",
        "\n",
        "---\n",
        "\n",
        "### 🧮 How to Estimate `begin_step` and `end_step`\n",
        "\n",
        "Each training **step** corresponds to one batch update. Use the following formulas to calculate the total number of training steps:\n",
        "\n",
        "$$ \\text{steps_per_epoch} = ceil\\Big(\\frac{\\text{num_train_samples}}{\\text{batch_size}}\\Big)$$\n",
        "\n",
        "$$\\text{total_steps} = \\text{steps_per_epoch} * \\text{num_epochs}$$\n",
        "\n",
        "\n",
        "**Example**:  \n",
        "If `num_train_samples = 60000`, `batch_size = 32`, and `num_epochs = 2`:\n",
        "\n",
        "$$ \\text{steps_per_epoch} = ceil\\Big(\\frac{60000}{32}\\Big) = 1875 $$\n",
        "\n",
        "$$\\text{total_steps} = 1875 * 2 = 3750$$\n",
        "\n",
        "You could then set:\n",
        "\n",
        "- `begin_step = 0`\n",
        "- `end_step = 2000`  _(pruning is applied during the first ~half of training)_"
      ],
      "metadata": {
        "id": "JhYd6u5ETkfK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow_model_optimization.sparsity.keras import (\n",
        "    prune_low_magnitude,\n",
        "    PolynomialDecay,\n",
        "    UpdatePruningStep\n",
        ")\n",
        "\n",
        "# Apply pruning to the model using a polynomial decay schedule\n",
        "def apply_pruning(model):\n",
        "    pruning_params = {\n",
        "        'pruning_schedule': PolynomialDecay(\n",
        "            initial_sparsity=0.5,   # Start pruning with 50% of weights set to zero\n",
        "            final_sparsity=0.9,     # Gradually increase sparsity to 90% by end_step\n",
        "            begin_step=0,           # Start pruning from the first training step\n",
        "            end_step=2000           # End pruning at step 2000\n",
        "        )\n",
        "    }\n",
        "    pruned_model = prune_low_magnitude(model, **pruning_params)\n",
        "    return pruned_model\n",
        "\n",
        "# Compile and train the pruned model\n",
        "pruned_model = apply_pruning(create_cnn_model())\n",
        "pruned_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Add the pruning update callback (required)\n",
        "callbacks = [UpdatePruningStep()]\n",
        "\n",
        "# Train the pruned model\n",
        "pruned_model.fit(\n",
        "    x_train, y_train,\n",
        "    epochs=2,\n",
        "    batch_size=32,\n",
        "    validation_data=(x_test, y_test),\n",
        "    callbacks=callbacks  # Important: ensures pruning step is updated during training\n",
        ")\n",
        "\n",
        "# Evaluate the pruned model\n",
        "pruned_accuracy = pruned_model.evaluate(x_test, y_test, verbose=0)[1]\n",
        "print(f\"Pruned Model Accuracy: {pruned_accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "aByTko35TjmY",
        "outputId": "ace77974-2802-444f-c247-5095b5bf3391"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'tensorflow_model_optimization'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-476573243.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m from tensorflow_model_optimization.sparsity.keras import (\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprune_low_magnitude\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mPolynomialDecay\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mUpdatePruningStep\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m )\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_model_optimization'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving the Pruned Model Without Removing the Pruning Mask\n",
        "Pruning introduces a pruning mask to track sparse connections. If we save the model without removing the mask, the model size does not reduce significantly.\n"
      ],
      "metadata": {
        "id": "e9QC_tWDTlyn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the pruned model without removing the pruning mask\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(pruned_model)\n",
        "tflite_model_with_mask = converter.convert()\n",
        "\n",
        "# Save the TFLite model\n",
        "with open(\"pruned_model_with_mask.tflite\", \"wb\") as f:\n",
        "    f.write(tflite_model_with_mask)\n",
        "\n",
        "print(\"Saved pruned model with pruning mask.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVTsDj0PTMGI",
        "outputId": "f175a5b3-90c3-4a7a-8614-0751071f5a66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _update_step_xla, conv2d_1_layer_call_fn, conv2d_1_layer_call_and_return_conditional_losses, _jit_compiled_convolution_op, flatten_1_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved pruned model with pruning mask.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Removing the Pruning Mask and Using Sparsity Optimization\n",
        "To reduce the model size, we must:\n",
        "1. Strip the pruning mask using `strip_pruning`.\n",
        "2. Use TensorFlow Lite's experimental sparsity-aware optimization.\n"
      ],
      "metadata": {
        "id": "F9h5PQUgYrqL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow_model_optimization.sparsity.keras import strip_pruning\n",
        "\n",
        "# Strip the pruning mask\n",
        "stripped_model = strip_pruning(pruned_model)\n",
        "\n",
        "# Convert the stripped model with experimental sparsity optimization\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(stripped_model)\n",
        "converter.optimizations = [tf.lite.Optimize.EXPERIMENTAL_SPARSITY]\n",
        "tflite_model_sparse = converter.convert()\n",
        "\n",
        "# Save the optimized sparse TFLite model\n",
        "with open(\"pruned_model_sparse.tflite\", \"wb\") as f:\n",
        "    f.write(tflite_model_sparse)\n",
        "\n",
        "print(\"Saved pruned model with experimental sparsity optimization.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T95ShBe3Yw8J",
        "outputId": "45a1270f-ae79-4cef-b70a-0034b54f419f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved pruned model with experimental sparsity optimization.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comparing Model Sizes and Accuracy\n",
        "We compare the sizes of:\n",
        "1. The baseline model.\n",
        "2. The pruned model without removing the pruning mask.\n",
        "3. The pruned model with experimental sparsity optimization.\n"
      ],
      "metadata": {
        "id": "-_adOYtnZBHE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Compare model sizes\n",
        "model_files = [\n",
        "    \"pruned_model_with_mask.tflite\",\n",
        "    \"pruned_model_sparse.tflite\"\n",
        "]\n",
        "\n",
        "print(\"\\nModel Sizes (KB):\")\n",
        "for file in model_files:\n",
        "    print(f\"{file}: {os.path.getsize(file) / 1024:.2f} KB\")\n",
        "\n",
        "# Evaluate accuracy for sparse TFLite model\n",
        "def evaluate_tflite_model(tflite_model_path):\n",
        "    interpreter = tf.lite.Interpreter(model_path=tflite_model_path)\n",
        "    interpreter.allocate_tensors()\n",
        "    input_details = interpreter.get_input_details()\n",
        "    output_details = interpreter.get_output_details()\n",
        "\n",
        "    correct_predictions = 0\n",
        "    for i in range(len(x_test)):\n",
        "        input_data = x_test[i:i+1].astype(\"float32\")\n",
        "        interpreter.set_tensor(input_details[0]['index'], input_data)\n",
        "        interpreter.invoke()\n",
        "        output_data = interpreter.get_tensor(output_details[0]['index'])\n",
        "        if tf.argmax(output_data, axis=1) == tf.argmax(y_test[i:i+1], axis=1):\n",
        "            correct_predictions += 1\n",
        "\n",
        "    return correct_predictions / len(x_test)\n",
        "\n",
        "# Print accuracies\n",
        "print(\"\\nModel Accuracies:\")\n",
        "print(f\"Pruned Model with Mask Accuracy: {pruned_accuracy:.4f}\")\n",
        "print(f\"Pruned Sparse Model Accuracy: {evaluate_tflite_model('pruned_model_sparse.tflite'):.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k42uW-A3ZBRO",
        "outputId": "6068dae1-a029-4d0f-b5f9-bf820985601a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model Sizes (KB):\n",
            "pruned_model_with_mask.tflite: 5427.45 KB\n",
            "pruned_model_sparse.tflite: 411.48 KB\n",
            "\n",
            "Model Accuracies:\n",
            "Pruned Model with Mask Accuracy: 0.9740\n",
            "Pruned Sparse Model Accuracy: 0.9740\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the baseline (unpruned) model to TensorFlow Lite format\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model_baseline = converter.convert()\n",
        "\n",
        "# Save the baseline TFLite model\n",
        "with open(\"baseline_model.tflite\", \"wb\") as f:\n",
        "    f.write(tflite_model_baseline)\n",
        "\n",
        "# Print the size of the baseline model\n",
        "import os\n",
        "baseline_model_size = os.path.getsize(\"baseline_model.tflite\") / 1024  # Size in KB\n",
        "print(f\"Baseline Model Size: {baseline_model_size:.2f} KB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F1unLQRtZvx1",
        "outputId": "b1740870-1f21-4571-a30b-b9f1445b9913"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline Model Size: 2713.34 KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part I Summary\n",
        "- Saving the pruned model without removing the pruning mask does not reduce the size.\n",
        "- Using experimental sparsity-aware optimization after stripping the pruning mask significantly reduces the model size while maintaining accuracy.\n",
        "- Pruning combined with sparsity is highly effective for compressing models for deployment on resource-constrained devices.\n"
      ],
      "metadata": {
        "id": "6QSjm0MUatVg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part II: Model Pruning + Quantization\n",
        "\n",
        "### Overview\n",
        "In this section, we combine **model pruning** and **quantization** to optimize a neural network for deployment on resource-constrained devices.\n",
        "\n",
        "- **Pruning** removes insignificant connections (weights) in the model, introducing sparsity, which can reduce model size and computational requirements.\n",
        "- **Quantization** reduces the precision of weights and activations, further compressing the model and enabling efficient inference.\n",
        "\n",
        "### Goals\n",
        "1. Apply pruning to the model to introduce sparsity.\n",
        "2. Explore the impact of:\n",
        "   - Saving the pruned model **without removing the pruning mask**.\n",
        "   - **Removing the pruning mask** and enabling **sparsity-aware optimizations**.\n",
        "3. Quantize the pruned model using **Float16 Quantization** and measure its impact on:\n",
        "   - Model size.\n",
        "   - Accuracy on the test set.\n",
        "\n",
        "### Key Points\n",
        "- **Pruning Masks:**  \n",
        "  Pruning introduces masks to track sparse connections in the model. These masks must be removed before final deployment to reduce size.\n",
        "  \n",
        "- **Sparsity-Aware Optimization:**  \n",
        "  TensorFlow Lite’s `EXPERIMENTAL_SPARSITY` optimization leverages the sparse structure of pruned models to significantly reduce storage and computation.\n",
        "\n",
        "- **Quantization:**  \n",
        "  Float16 quantization reduces model size by converting all float32 weights to float16 while keeping activations in float32. This allows the model to remain accurate while benefiting from size reduction—particularly effective on hardware that supports float16 computations (e.g., GPUs, NPUs).\n",
        "\n",
        "### Steps Demonstrated\n",
        "1. Prune the model and save it **with the pruning mask**.\n",
        "2. Remove the pruning mask and apply **sparsity-aware optimization**.\n",
        "3. Quantize the pruned models (both with and without the mask) using **Float16 Quantization**.\n",
        "4. Compare the model sizes and test accuracies across:\n",
        "   - Pruned Model with Mask  \n",
        "   - Pruned Model with Sparsity Optimization  \n",
        "   - Float16 Quantized Models\n"
      ],
      "metadata": {
        "id": "uqja0olwgV1H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### 🧪 Lab 2 Submission Reminder\n",
        "\n",
        "**Please complete the code below and take a screenshot of it as part of your Lab 2 submission.**\n",
        "\n",
        "⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "RU0jw4CC5Fy-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Float16 Quantization for Pruned Model with Mask\n",
        "\n",
        "converter = #<--- Enter Your Code Here --->#\n",
        "converter.optimizations = #<--- Enter Your Code Here --->#\n",
        "converter.target_spec.supported_types = #<--- Enter Your Code Here --->#\n",
        "tflite_model_with_mask_float16 = #<--- Enter Your Code Here --->#\n",
        "\n",
        "# Save the quantized model\n",
        "with open(\"pruned_model_with_mask_float16.tflite\", \"wb\") as f:\n",
        "    f.write(tflite_model_with_mask_float16)\n",
        "print(\"Saved Float16 Quantized Model (with Mask).\")\n",
        "\n",
        "# Float16 Quantization for Stripped Model\n",
        "\n",
        "converter = #<--- Enter Your Code Here --->#\n",
        "# Make sure to apply appropriate optimizations to account for both pruning and quantization\n",
        "converter.optimizations = #<--- Enter Your Code Here --->#\n",
        "converter.target_spec.supported_types = #<--- Enter Your Code Here --->#\n",
        "tflite_model_sparse_float16 = #<--- Enter Your Code Here --->#\n",
        "\n",
        "# Save the quantized model\n",
        "with open(\"pruned_model_sparse_float16.tflite\", \"wb\") as f:\n",
        "    f.write(tflite_model_sparse_float16)\n",
        "print(\"Saved Float16 Quantized Model (Stripped).\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "E5tfiR7Ibbrt",
        "outputId": "d9a85f3e-5301-429a-8732-1a4ee91935a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-2775117757.py, line 3)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-2775117757.py\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    converter = #<--- Enter Your Code Here --->#\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Compare model sizes after full integer quantization\n",
        "quantized_model_files = [\n",
        "    \"pruned_model_with_mask_float16.tflite\",\n",
        "    \"pruned_model_sparse_float16.tflite\"\n",
        "]\n",
        "\n",
        "print(\"\\nModel Sizes After Float16 Quantization (KB):\")\n",
        "for file in quantized_model_files:\n",
        "    print(f\"{file}: {os.path.getsize(file) / 1024:.2f} KB\")\n",
        "\n",
        "# Evaluate accuracy of a TFLite model on the test set\n",
        "def evaluate_tflite_model(tflite_model_path):\n",
        "    # Load the TFLite model and allocate tensors\n",
        "    interpreter = tf.lite.Interpreter(model_path=tflite_model_path)\n",
        "    interpreter.allocate_tensors()\n",
        "\n",
        "    # Get input and output tensor details\n",
        "    input_details = interpreter.get_input_details()\n",
        "    output_details = interpreter.get_output_details()\n",
        "\n",
        "    input_dtype = input_details[0]['dtype']\n",
        "    input_scale, input_zero_point = input_details[0]['quantization']\n",
        "\n",
        "    correct_predictions = 0\n",
        "\n",
        "    for i in range(len(x_test)):\n",
        "        # Prepare one input sample (shape: [1, 28, 28, 1])\n",
        "        input_data = x_test[i:i+1].astype(\"float32\")\n",
        "\n",
        "        # Quantize the input if model expects int8\n",
        "        if input_dtype == np.int8:\n",
        "            input_data = input_data / input_scale + input_zero_point\n",
        "            input_data = np.round(input_data).astype(np.int8)\n",
        "        elif input_dtype == np.float16:\n",
        "            input_data = input_data.astype(np.float16)\n",
        "\n",
        "        # Set the tensor and invoke the interpreter\n",
        "        interpreter.set_tensor(input_details[0]['index'], input_data)\n",
        "        interpreter.invoke()\n",
        "\n",
        "        # Get prediction and compare to true label\n",
        "        output_data = interpreter.get_tensor(output_details[0]['index'])\n",
        "        predicted_label = np.argmax(output_data, axis=1)\n",
        "        true_label = np.argmax(y_test[i:i+1], axis=1)\n",
        "\n",
        "        if predicted_label == true_label:\n",
        "            correct_predictions += 1\n",
        "\n",
        "    # Return classification accuracy\n",
        "    return correct_predictions / len(x_test)\n",
        "\n",
        "\n",
        "# Evaluate accuracy for quantized TFLite models\n",
        "accuracy_with_mask = evaluate_tflite_model(\"pruned_model_with_mask_float16.tflite\")\n",
        "accuracy_sparse = evaluate_tflite_model(\"pruned_model_sparse_float16.tflite\")\n",
        "\n",
        "print(f\"Pruned Model with Mask Accuracy + Float16 Quantization Accuracy: {accuracy_with_mask:.4f}\")\n",
        "print(f\"Pruned Sparse Model + Float16 Quantization Accuracy: {accuracy_sparse:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "id": "ytCAXIwvhLOj",
        "outputId": "e2db7a8c-17c2-4dcb-b15a-080e6dc2f8d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model Sizes After Full Integer Quantization (KB):\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'os' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-da942de717c8>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nModel Sizes After Full Integer Quantization (KB):\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mquantized_model_files\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{file}: {os.path.getsize(file) / 1024:.2f} KB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mevaluate_tflite_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtflite_model_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️\n",
        "\n",
        "### 📸 Lab 2 Submission Reminder\n",
        "\n",
        "**Please take a screenshot of the above result and include it as part of your Lab 2 submission.**\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "sZh7pzFt5KCr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part II Summary\n",
        "- **Model Size Comparison**:\n",
        "  - Float 16 Quantization on the pruned model with the mask does not achieve optimal compression.\n",
        "  - Float 16 Quantization on the stripped sparse model results in a much smaller size due to the removal of the pruning mask and sparsity-aware optimization.\n",
        "\n",
        "- **Accuracy Comparison**:\n",
        "  - Both models achieve similar accuracy on the test set, demonstrating that stripping the pruning mask does not affect performance while only affecting optimizing the model size.\n",
        "\n",
        "This demonstrates the importance of properly finalizing a pruned model for deployment.\n",
        "\n"
      ],
      "metadata": {
        "id": "lMfDvq99ar1s"
      }
    }
  ]
}